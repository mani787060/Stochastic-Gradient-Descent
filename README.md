# Stochastic Gradient Descent (SGD)

-> This project explains and implements **Stochastic Gradient Descent**, the fastest and most noisy version of gradient descent.  
-> It includes parameter updates using one sample at a time and a comparison with sklearnâ€™s SGDRegressor.

## Contents:-
-> SGD implemented **from scratch using NumPy**
-> SGD using **sklearn SGDRegressor**
-> Random sample selection for every iteration
-> Loss visualization over multiple epochs

## Key Concepts:-
-> Difference between Batch GD, Mini-Batch GD, and Stochastic GD
-> Why SGD converges faster (but noisier)
-> Effect of randomness on the optimization path

## Libraries Used:-
-> NumPy  
-> Pandas  
-> Matplotlib  
-> Scikit-learn
